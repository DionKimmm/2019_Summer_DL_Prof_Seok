{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "190731_RNN_Seq2seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DionKimmm/2019_Summer_DL_Prof_Seok/blob/master/190731_RNN_Seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eKvo4CsUjWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hRA-eiIWel_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# S: 디코딩 입력의 시작을 나타내는 심볼\n",
        "# E: 디코딩 출력을 끝을 나타내는 심볼\n",
        "# P: 현재 배치 데이터의 time step 크기보다 작은 경우 빈 시퀀스를 채우는 심볼\n",
        "#    예) 현재 배치 데이터의 최대 크기가 4 인 경우\n",
        "#       word -> ['w', 'o', 'r', 'd']\n",
        "#       to   -> ['t', 'o', 'P', 'P']\n",
        "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀키스사랑']\n",
        "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
        "dic_len = len(num_dic)\n",
        "\n",
        "# 영어를 한글로 번역하기 위한 학습 데이터\n",
        "seq_data = [['word', '단어'], ['wood', '나무'], ['game', '놀이'], ['girl', '소녀'], ['kiss', '키스'], ['love', '사랑']]\n",
        "\n",
        "def make_batch(seq_data):\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    target_batch = []\n",
        "\n",
        "    for seq in seq_data:\n",
        "        # 인코더 셀의 입력값. 입력단어의 글자들을 한글자씩 떼어 배열로 만든다.\n",
        "        input = [num_dic[n] for n in seq[0]]\n",
        "        # 디코더 셀의 입력값. 시작을 나타내는 S 심볼을 맨 앞에 붙여준다.\n",
        "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
        "        # 학습을 위해 비교할 디코더 셀의 출력값. 끝나는 것을 알려주기 위해 마지막에 E 를 붙인다.\n",
        "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
        "\n",
        "        input_batch.append(np.eye(dic_len)[input])\n",
        "        output_batch.append(np.eye(dic_len)[output])\n",
        "        target_batch.append(target)   # 출력값만 one-hot 인코딩이 아님 \n",
        "\n",
        "    return input_batch, output_batch, target_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El1Pm_SrWieX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7a59d374-c50c-43aa-f77e-17caf038aea0"
      },
      "source": [
        "print(seq_data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['word', '단어'], ['wood', '나무'], ['game', '놀이'], ['girl', '소녀'], ['kiss', '키스'], ['love', '사랑']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDCdyKB4W4vW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b0b94ce5-3039-4b83-914d-a33ec997474e"
      },
      "source": [
        "print(seq_data[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['word', '단어']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3fASpeTW6wn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 옵션 설정\n",
        "learning_rate = 0.01\n",
        "n_hidden = 128\n",
        "total_epoch = 100\n",
        "# 입력과 출력의 형태가 one-hot 인코딩으로 같으므로 크기도 같다.\n",
        "n_class = n_input = dic_len\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFs9aGSoYFqF",
        "colab_type": "text"
      },
      "source": [
        "# 신경망 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlaXkiYpYDMU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "135a4fbb-6ecd-486f-aaff-8d1e8d0be1de"
      },
      "source": [
        "# Seq2Seq 모델은 인코더의 입력과 디코더의 입력의 형식이 같다.\n",
        "# [batch size, time steps, input size]\n",
        "enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "# [batch size, time steps]\n",
        "targets = tf.placeholder(tf.int64, [None, None])\n",
        "\n",
        "# 인코더 셀을 구성한다.\n",
        "with tf.variable_scope('encode'):\n",
        "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
        "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input, dtype=tf.float32)\n",
        "\n",
        "# 디코더 셀을 구성한다.\n",
        "with tf.variable_scope('decode'):\n",
        "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
        "\n",
        "    # Seq2Seq 모델은 인코더 셀의 최종 상태값을 디코더 셀의 초기 상태값으로 넣어주는 것이 핵심.\n",
        "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input, initial_state=enc_states, dtype=tf.float32)\n",
        "\n",
        "model = tf.layers.dense(outputs, n_class, activation=None)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0731 04:53:55.233685 139707418855296 deprecation.py:323] From <ipython-input-7-339d3efbbc30>:8: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0731 04:53:55.238354 139707418855296 deprecation.py:323] From <ipython-input-7-339d3efbbc30>:10: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0731 04:53:55.298926 139707418855296 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0731 04:53:55.310224 139707418855296 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:459: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0731 04:53:55.532647 139707418855296 deprecation.py:323] From <ipython-input-7-339d3efbbc30>:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P_GlkGRYJXj",
        "colab_type": "text"
      },
      "source": [
        "# 손실함수 및 최적화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2knvMKrIYMwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=targets))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhvv81ehYNuE",
        "colab_type": "text"
      },
      "source": [
        "# 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udmXOMw0YPSW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "89a39397-8d96-44f1-a11d-3c72588b92b8"
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "input_batch, output_batch, target_batch = make_batch(seq_data)\n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "    _, loss = sess.run([optimizer, cost],\n",
        "                       feed_dict={enc_input: input_batch,\n",
        "                                  dec_input: output_batch,\n",
        "                                  targets: target_batch})\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1),\n",
        "          'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "print('최적화 완료!')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 3.711872\n",
            "Epoch: 0002 cost = 2.552446\n",
            "Epoch: 0003 cost = 1.568520\n",
            "Epoch: 0004 cost = 1.123368\n",
            "Epoch: 0005 cost = 0.569112\n",
            "Epoch: 0006 cost = 0.434763\n",
            "Epoch: 0007 cost = 0.438927\n",
            "Epoch: 0008 cost = 0.390502\n",
            "Epoch: 0009 cost = 0.262222\n",
            "Epoch: 0010 cost = 0.210280\n",
            "Epoch: 0011 cost = 0.056764\n",
            "Epoch: 0012 cost = 0.063888\n",
            "Epoch: 0013 cost = 0.094253\n",
            "Epoch: 0014 cost = 0.114470\n",
            "Epoch: 0015 cost = 0.075624\n",
            "Epoch: 0016 cost = 0.030633\n",
            "Epoch: 0017 cost = 0.047901\n",
            "Epoch: 0018 cost = 0.058239\n",
            "Epoch: 0019 cost = 0.080011\n",
            "Epoch: 0020 cost = 0.010707\n",
            "Epoch: 0021 cost = 0.020266\n",
            "Epoch: 0022 cost = 0.014998\n",
            "Epoch: 0023 cost = 0.035679\n",
            "Epoch: 0024 cost = 0.009661\n",
            "Epoch: 0025 cost = 0.013366\n",
            "Epoch: 0026 cost = 0.018973\n",
            "Epoch: 0027 cost = 0.023194\n",
            "Epoch: 0028 cost = 0.007676\n",
            "Epoch: 0029 cost = 0.002439\n",
            "Epoch: 0030 cost = 0.009545\n",
            "Epoch: 0031 cost = 0.002766\n",
            "Epoch: 0032 cost = 0.002087\n",
            "Epoch: 0033 cost = 0.008121\n",
            "Epoch: 0034 cost = 0.004121\n",
            "Epoch: 0035 cost = 0.008224\n",
            "Epoch: 0036 cost = 0.001901\n",
            "Epoch: 0037 cost = 0.005052\n",
            "Epoch: 0038 cost = 0.003373\n",
            "Epoch: 0039 cost = 0.001907\n",
            "Epoch: 0040 cost = 0.001199\n",
            "Epoch: 0041 cost = 0.001233\n",
            "Epoch: 0042 cost = 0.049904\n",
            "Epoch: 0043 cost = 0.003388\n",
            "Epoch: 0044 cost = 0.001420\n",
            "Epoch: 0045 cost = 0.009540\n",
            "Epoch: 0046 cost = 0.002355\n",
            "Epoch: 0047 cost = 0.001017\n",
            "Epoch: 0048 cost = 0.001384\n",
            "Epoch: 0049 cost = 0.003744\n",
            "Epoch: 0050 cost = 0.000812\n",
            "Epoch: 0051 cost = 0.000952\n",
            "Epoch: 0052 cost = 0.002018\n",
            "Epoch: 0053 cost = 0.000454\n",
            "Epoch: 0054 cost = 0.001933\n",
            "Epoch: 0055 cost = 0.004380\n",
            "Epoch: 0056 cost = 0.000847\n",
            "Epoch: 0057 cost = 0.002165\n",
            "Epoch: 0058 cost = 0.002206\n",
            "Epoch: 0059 cost = 0.000680\n",
            "Epoch: 0060 cost = 0.000866\n",
            "Epoch: 0061 cost = 0.000783\n",
            "Epoch: 0062 cost = 0.001219\n",
            "Epoch: 0063 cost = 0.000753\n",
            "Epoch: 0064 cost = 0.001005\n",
            "Epoch: 0065 cost = 0.000613\n",
            "Epoch: 0066 cost = 0.000703\n",
            "Epoch: 0067 cost = 0.000684\n",
            "Epoch: 0068 cost = 0.000877\n",
            "Epoch: 0069 cost = 0.000485\n",
            "Epoch: 0070 cost = 0.000272\n",
            "Epoch: 0071 cost = 0.000819\n",
            "Epoch: 0072 cost = 0.000975\n",
            "Epoch: 0073 cost = 0.000664\n",
            "Epoch: 0074 cost = 0.001525\n",
            "Epoch: 0075 cost = 0.000423\n",
            "Epoch: 0076 cost = 0.000553\n",
            "Epoch: 0077 cost = 0.000499\n",
            "Epoch: 0078 cost = 0.000747\n",
            "Epoch: 0079 cost = 0.000719\n",
            "Epoch: 0080 cost = 0.000277\n",
            "Epoch: 0081 cost = 0.004538\n",
            "Epoch: 0082 cost = 0.000820\n",
            "Epoch: 0083 cost = 0.000462\n",
            "Epoch: 0084 cost = 0.000940\n",
            "Epoch: 0085 cost = 0.000240\n",
            "Epoch: 0086 cost = 0.001512\n",
            "Epoch: 0087 cost = 0.000973\n",
            "Epoch: 0088 cost = 0.000624\n",
            "Epoch: 0089 cost = 0.001128\n",
            "Epoch: 0090 cost = 0.000470\n",
            "Epoch: 0091 cost = 0.000640\n",
            "Epoch: 0092 cost = 0.000298\n",
            "Epoch: 0093 cost = 0.000160\n",
            "Epoch: 0094 cost = 0.000822\n",
            "Epoch: 0095 cost = 0.000531\n",
            "Epoch: 0096 cost = 0.001417\n",
            "Epoch: 0097 cost = 0.000335\n",
            "Epoch: 0098 cost = 0.000219\n",
            "Epoch: 0099 cost = 0.000108\n",
            "Epoch: 0100 cost = 0.000148\n",
            "최적화 완료!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgr-VKeTYSuj",
        "colab_type": "text"
      },
      "source": [
        "# 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6OGYUY0YVpk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "6afe58fc-00d8-4b79-dc66-8e347c96abb2"
      },
      "source": [
        "# 단어를 입력받아 번역 단어를 예측하고 디코딩하는 함수\n",
        "def translate(word):\n",
        "    # 이 모델은 입력값과 출력값 데이터로 [영어단어, 한글단어] 사용하지만,\n",
        "    # 예측시에는 한글단어를 알지 못하므로, 디코더의 입출력값을 의미 없는 값인 P 값으로 채운다.\n",
        "    # ['word', 'PPPP']\n",
        "    seq_data = [word, 'P' * len(word)]\n",
        "    input_batch, output_batch, target_batch = make_batch([seq_data])\n",
        "\n",
        "    # 결과가 [batch size, time step, input] 으로 나오기 때문에,\n",
        "    # 2번째 차원인 input 차원을 argmax 로 취해 가장 확률이 높은 글자를 예측 값으로 만든다.\n",
        "    prediction = tf.argmax(model, 2)\n",
        "\n",
        "    result = sess.run(prediction, feed_dict={enc_input: input_batch, dec_input: output_batch, targets: target_batch})\n",
        "\n",
        "    # 결과 값인 숫자의 인덱스에 해당하는 글자를 가져와 글자 배열을 만든다.\n",
        "    decoded = [char_arr[i] for i in result[0]]\n",
        "\n",
        "    # 출력의 끝을 의미하는 'E' 이후의 글자들을 제거하고 문자열로 만든다.\n",
        "    end = decoded.index('E')\n",
        "    translated = ''.join(decoded[:end])\n",
        "\n",
        "    return translated\n",
        "\n",
        "print('\\n=== 번역 테스트 ===')\n",
        "\n",
        "print('word ->', translate('word'))\n",
        "print('wodr ->', translate('wodr'))\n",
        "print('love ->', translate('love'))\n",
        "print('loev ->', translate('loev'))\n",
        "print('abcd ->', translate('abcd'))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=== 번역 테스트 ===\n",
            "word -> 단어\n",
            "wodr -> 나무\n",
            "love -> 사랑\n",
            "loev -> 사랑\n",
            "abcd -> 사스사이\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMT2DgZaYV7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}